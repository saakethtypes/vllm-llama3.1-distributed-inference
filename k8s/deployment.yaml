apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-inference
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: llama-inference
        image: llama-inference:latest
        resources:
          limits:
            nvidia.com/gpu: 4  # Limit to 4 GPUs
        env:
        - name: TENSOR_PARALLEL_SIZE
          value: "4"
        - name: PIPELINE_PARALLEL_SIZE
          value: "1"
        ports:
        - containerPort: 8000

apiVersion: v1
kind: Service
metadata:
  name: llama-inference-service
spec:
  selector:
    app: llama-inference
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8000
  type: LoadBalancer
