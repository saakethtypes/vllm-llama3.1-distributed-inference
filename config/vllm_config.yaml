# Model settings
model_path: "LLaMA-3.1"      
tensor_parallel_size: 4      
pipeline_parallel_size: 1     

# Serving settings
max_tokens: 2048             
batch_size: 8               
cache_size: 128               # Cache size to optimize repeated prompts

logging_level: "INFO"    
